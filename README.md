# A pipeline for generating synthetic multi-label datasets based on topic models

## What is multi label classificaiton?
Multi-label classification refers to the process of building algorithms that can annotate an item, such as a user query with the most relevant subset of classes. When the number of classes run into the millions it is referred to [extreme classification](http://manikvarma.org/downloads/XC/XMLRepository.html) 

## What is the need for synthetic datasets?
Multi label applications belong to a wide variety of scenarios whose dataset properties can differ significantly from one another. Some such properties could be the degree of label skew, the amount of missing labels, the extent of feature heterogeneity etc. Unfortunately, the available [benchmarks](http://manikvarma.org/downloads/XC/XMLRepository.html) are limited in their coverage of these settings. Furthermore, properties such as missing labels and label skew, both of which impact tail performance, always co-occur in practical applications with missing labels being especially hard to identify. As a result, isolating and studying their individual effects on different algorithms is infeasible on real datasets. This significantly limits the development of powerful solutions which are applicable generally and not just under specific settings. Synthetic datasets offer a principled way to exhaustively create various dataset settings and study their individual or relative impact on al algorithm's performance.

## Topic model based Synthetic dataset generation framework

The repository provides an implementation of a topic modeling algorithm, Prior-LDA for generating multi-label data, to generate synthetic XML datasets. Prior-LDA involves 4 entities: documents, labels, tokens and topics. The documents and labels contain a set of unordered tokens representing their content. A topic is defined as a probabilistic distribution over tokens. For example, a sports-related topic might generate tokens like “football” or “athlete”. A hyper-parameter κ controls the prevalence of each topic in the dataset. In Prior-LDA, there exists a 1:1 correspondence between a topic and a label. A document is generated as follows: For each document, iteratively sample a topic and then sample a token from it until all document tokens are generated. The relevant labels for each document are sampled from the set of all topics triggered by it. In this generation, missing label property is simulated as follows. A chosen topic can be accepted or rejected with probability ν (a hyper-parameter) where rejected topics don’t participate in sampling the ground truth labels but continue to be used for generating document tokens as usual. Finally, the label text is generated by sampling tokens from its associated topic.
